---
title: "Linear Regressions"
author: "Congxin (David) Xu"
date: "1/5/2021"
output: html_document
---


```{r Setup, include=FALSE}
knitr::opts_chunk$set(error=TRUE,           # Keep compiling upon error
                      collapse=FALSE,       # collapse by default
                      echo=TRUE,            # echo code by default
                      comment = "#>",       # change comment character
                      fig.width = 5,        # set figure width
                      fig.align = "center", # set figure position
                      out.width = "49%",    # set width of displayed images
                      warning=TRUE,         # show R warnings
                      message=TRUE)         # show R messages
options(dplyr.summarise.inform = FALSE)     # ignore message about group structure
```


```{css Solution-Region, echo=FALSE}
.solution {
  background-color: #232D4B10;
  border-style: solid;
  border-color: #232D4B;
  padding: .5em;
  margin: 20px
}
```


## Description

This tutorial is going to discuss how to implement linear regressions in `R`. We are going to cover:

- Ordinary Least Squares Regression
- Step-wise Regression
- Penalized Linear Regression
  - Lasso Regression
  - Ridge Regression
  - Elastic Net Regression

## Package Dependency

- [`tidyverse`](https://www.tidyverse.org/)
  - We will mainly use `tidyverse` for data manipulation (`dplyr`) and visualization (`ggplot2`). 
- [`broom`](https://cran.r-project.org/web/packages/broom/broom.pdf)
  - We will mainly use the 'tidy()' function from broom to print out the cleaned summary of the model information.
- [`stats`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html)
  - `stats` package is part of the base R package, so you do not need to install it. It should already be pre-installed.
  - This is the package that contains the `lm()` function that will perform the linear regression and the `step()` function that will perform the stepwise regression.
- [`glmnet`](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf)
  - Title: Lasso and Elastic-Net Regularized Generalized Linear Models
  - This is the package that contains the functions that will perform ridge, lasso, and elastic net regressions. 

  
## Use Case

- Linear Regression models assume the linear relationship between the response variable and the predictors. It can be used to solve almost all regression type of problems.

## Caution

- If you care more about the inference of the model or the interpretation of the model, you need to pay attention to the potential violation of the assumptions of linear regression models. 
- If you care more about the predictive power of the model, you need to pay attention to the accuracy of the model.

## Tutorial

<div class="solution">
Load the required library
```{r Packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)
library(glmnet)
```
</div>


The data we will use is the housing price data from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).

- Response Variable: **`price`**

<div class="solution">
**Read and Preview the Training Data**
```{r Training Data, message=FALSE}
train <- read_csv('.\\Data\\realestate-train.csv')
head(train)
```

**Read and Preview the Testing Data**
```{r Testing Data, message=FALSE}
test <- read_csv('.\\Data\\realestate-test.csv')
head(test)
```
```{r Cleaning, include = FALSE}
# train <- train %>% 
#   select(price, SqFeet, Age, Baths, TotRmsAbvGrd, BldgType) %>% 
#   mutate(BldgType = as.numeric(as.factor(BldgType)))
# test <- test %>% 
#   select(SqFeet, Age, Baths, TotRmsAbvGrd, BldgType) %>% 
#   mutate(BldgType = as.numeric(as.factor(BldgType)))
```

</div>


### Ordinary Least Squares Regression
<div class="solution">
**Assumptions**

1. The errors, for each fixed value of $x$, have mean 0.
2. The errors, for each fixed value of $x$, have constant variance.
3. The errors are independent.
4. The errors, for each fixed value of $x$, follow a normal distribution.

**For this section, we will just focus on the following predictors:**

- `SqFeet`: *numeric*
- `Age`: *numeric*
- `Baths`: *numeric*
- `TotRmsAbvGrd`: *numeric*
- `BldgType`: *categorical*

**Set Up the Model**

```{r}
linear_model <- lm(data = train, price ~ SqFeet + Age + Baths + TotRmsAbvGrd + BldgType)
summary(linear_model)
```
**Report Mean Squared Error on Training Data**

```{r}
mean((linear_model$residuals)^2)
```

</div>

### Stepwise Regression Model
<div class="solution">
- For the stepwise regression model, we are going to try to include all predictors in the training data and see what parameters will be picked out based on the AIC.

```{r}
stepwise_model <- step(lm(data = train, price ~ .),
                     direction="both")
```

*Report the final parameters of stepwise model*
```{r}
summary(stepwise_model)
```
**Report Mean Squared Error on Training Data**

```{r}
mean((stepwise_model$residuals)^2)
```
</div>

### Penalized Linear Regression

- $\lambda$ is the penalty on including additional variables to the model.
- We will first use `cv.glmnet()` function to determine the best penalty for this model using cross validation.

#### Lasso Regression 

- **Lasso tends to shrink the coefficient of the predictors all the way to 0**.
- For the Lasso Regression, we need to set **`alpha = 1`**.

<div class="solution">


```{r}
# Control Randomness
set.seed(666)

# Initiate the Cross Validation 
lasso_cv_model <- cv.glmnet(x = train %>% 
                              select(-price,-CentralAir,-BldgType,-HouseStyle) %>% 
                              as.matrix(),
                            y = train %>% select(price) %>% as.matrix(),
                            alpha = 1, # Lasso
                            nfolds = 10, # 10 fold cross validation
                            family = 'gaussian') # use "gaussian" or "poisson" for regression problems

# Visualize the Lasso Model with Cross Validation
plot(lasso_cv_model)

# Report the Cross Validation MSE at lambda.1se
print(lasso_cv_model$cvm[which(lasso_cv_model$lambda == lasso_cv_model$lambda.1se)])
```
- Then, we will use the $\lambda$ at 1 standard deviation (the 2nd dotted line above) as an input to the final `glmnet()` model.

```{r}
# Training the final lasso model for prediction
lasso_model <- glmnet(x = train %>% 
                        select(-price,-CentralAir,-BldgType,-HouseStyle) %>% 
                        as.matrix(),
                      y = train %>% select(price) %>% as.matrix(),
                      alpha = 1,
                      lambda = lasso_cv_model$lambda.1se, # lambda.1se is obtained from previous step
                      family = 'gaussian')

# Display the coefficients
print(lasso_model$beta)
```

Based on the output above, we can see that `PoolArea`, `TotRmsAbvGrd` and `Baths` are removed as unhelpful predictors.
</div>

#### Ridge Regression 

- **Ridge tends to keep all the predictors.**.
- For the Ridge Regression, we need to set **`alpha = 0`**.

<div class="solution">
```{r}
# Control Randomness
set.seed(666)

# Initiate the Cross Validation 
ridge_cv_model <- cv.glmnet(x = train %>% 
                              select(-price,-CentralAir,-BldgType,-HouseStyle) %>% 
                              as.matrix(),
                            y = train %>% select(price) %>% as.matrix(),
                            alpha = 0, # Ridge
                            nfolds = 10, # 10 fold cross validation
                            family = 'gaussian') # use "gaussian" or "poisson" for regression problems

# Visualize the Lasso Model with Cross Validation
plot(ridge_cv_model)

# Report the Cross Validation MSE at lambda.1se
print(ridge_cv_model$cvm[which(ridge_cv_model$lambda == ridge_cv_model$lambda.1se)])
```

```{r}
# Training the final lasso model for prediction
ridge_model <- glmnet(x = train %>% 
                        select(-price,-CentralAir,-BldgType,-HouseStyle) %>% 
                        as.matrix(),
                      y = train %>% select(price) %>% as.matrix(),
                      alpha = 0,
                      lambda = lasso_cv_model$lambda.1se, # lambda.1se is obtained from previous step
                      family = 'gaussian')

# Display the coefficients
print(ridge_model$beta)
```
Based on the output above, we can see that all predictors have non-zero coefficients in this Ridge model.

</div>


#### Elastic Net Regression 

- For Elastic Net, we have 2 tuning parameters `alpha` and `lambda`. 
- We need to find the best `alpha` value using a for loop with cross validation. 
- Then, we need to find the best lambda based on the best alpha value. 

<div class="solution">
```{r}
# Control Randomness
set.seed(666) 

# Pre-defined setting
alpha <- seq(0, 1, 0.01)

# Create a empty list to store results
output <- list()

# Finding the best alpha
for (a in 1:length(alpha)) {

    # Build model using CV 
  model <- cv.glmnet(x = train %>% 
                       select(-price, -CentralAir, -BldgType, -HouseStyle) %>% 
                       as.matrix(),
                     y = train %>% select(price) %>% as.matrix(), 
                     nfolds = 10, 
                     alpha = alpha[a],
                     family = 'gaussian')
    
    # Store the data into output
    output[[a]] <- data.frame(alpha = alpha[a], 
                              lambda = model$lambda.1se,
                              # cvm is an output stored within the model object
                              # it is the cross validation mean squared error
                              mse = model$cvm[which(model$lambda == model$lambda.1se)])
  
}

# Convert list back to data frame
output <- bind_rows(output)

# Report alpha and lambda for final prediction
output %>% 
  filter(mse == min(mse))
```

```{r}
# Collect the best output
best <- output %>% filter(mse == min(mse))

# Training the final lasso model for prediction
elastic_net_model <- glmnet(x = train %>% 
                              select(-price, -CentralAir, -BldgType, -HouseStyle) %>% 
                              as.matrix(),
                            y = train %>% select(price) %>% as.matrix(),
                            alpha = best$alpha,
                            lambda = best$lambda, # obtained from the table above
                            family = 'gaussian')

# Display the coefficients
print(elastic_net_model$beta)
```
Based on the output above, we can see that `PoolArea`, `TotRmsAbvGrd` and `Baths` are removed as unhelpful predictors.

</div>

### Making Predictions

In this section, we are going to predict `price` on the test data using all 5 models generated above.
<div class="solution">
```{r}
test <- test %>%
  mutate(
    # Predict the future using linear regression 
    price_linear_pred = predict(linear_model, newdata = test),
    
    # Predict the future using stepwise regression 
    price_step_pred = predict(stepwise_model, newdata = test),
    
    # Predict the future using lasso regression 
    price_lasso_pred = predict(
      lasso_model,
      newx = test %>%
        select(-CentralAir,-BldgType,-HouseStyle) %>%
        as.matrix()
    ),
    
    # Predict the future using ridge regression 
    price_ridge_pred = predict(
      ridge_model,
      newx = test %>%
        select(-CentralAir,-BldgType,-HouseStyle) %>%
        as.matrix()
    ),
    
    # Predict the future using elastic net regression 
    price_enet_pred = predict(
      elastic_net_model,
      newx = test %>%
        select(-CentralAir,-BldgType,-HouseStyle) %>%
        as.matrix()
    )
  )
```
</div>